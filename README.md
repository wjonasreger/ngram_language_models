# Traditional n-Gram Language Models

In this notebook, some traditional n-gram language models are trained on the WikiText-2 data, a corpus of high-quality Wikipedia articles. The dataset was originally introduced in the following [paper](https://arxiv.org/pdf/1609.07843v1.pdf). A raw version of the data can easily be viewed [here](https://github.com/pytorch/examples/tree/master/word_language_model/data/wikitext-2).

---

I implemented four types of language models: two unigram models and two bigram models. Each pair has an unsmoothed and smoothed model. Each model class contains the three class methods shown below.

* **Sentence generation** (i.e., `generateSentence(self)`): Returns a sentence that is generated by the model. The output is a list of words that are found in the vocab set, including the `UNK` token but excluding the `START` and `END` tokens. Each sentence starts with the `START` token (with probability 1). The number of words in a sentence is not fixed, rather it terminates whenever `END` is generated. The probability of an n-gram is estimated by $\hat{P}(n_i)$, where $n_i$ is an n-gram.

* **Log-likelihood of a Sentence** (i.e., `getSentenceLogLikelihood(self, sentence)`): Returns the logarithm of the likelihood probability of `sentence`, which is a list of words that exist in the vocab set. The log-likelihood of a sentence is estimated by $\sum_{i=1}^n \log P(n_i)$, where $n$ is the length of a sentence.

* **Corpus Perplexity** (i.e., `getCorpusPerplexity(self, test_data)`): Computes the perplexity (i.e., normalized inverse log probability) of `test_data`. The corpus perplexity is estimated by $\exp \left( -\frac1N \times \sum_{i=1}^N \log P(n_i) \right)$