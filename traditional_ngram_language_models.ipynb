{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZKt-WVr6OtP4"
   },
   "source": [
    "# Traditional n-Gram Language Models\n",
    "\n",
    "In this notebook, some traditional n-gram language models are trained on the WikiText-2 data, a corpus of high-quality Wikipedia articles. The dataset was originally introduced in the following [paper](https://arxiv.org/pdf/1609.07843v1.pdf). A raw version of the data can easily be viewed [here](https://github.com/pytorch/examples/tree/master/word_language_model/data/wikitext-2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B28d_QB0kJhq"
   },
   "source": [
    "## Load Packages\n",
    "\n",
    "The `torchdata` package will need to be installed prior to running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zOxc8J3NkIUy",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jonas\\anaconda3\\envs\\qsite\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torchtext\n",
    "import random\n",
    "import sys\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "random.seed(22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cN2Ja8MNP4qS"
   },
   "source": [
    "## Preprocess Data\n",
    "\n",
    "To make these models more robust, it is necessary to perform some basic preprocessing on the corpora.\n",
    "\n",
    "* **Sentence splitting:** The interest behind this procedure is to model individual sentences, rather than longer chunks of text such as paragraphs or documents. The WikiText-2 dataset provides paragraphs; thus, a simple method is used to identify individual sentences by splitting paragraphs at punctuation tokens (\".\",  \"!\",  \"?\").\n",
    "\n",
    "* **Sentence markers:** For both training and testing corpora, each sentence is surrounded by a start-of-sentence (`START`) and end-of-sentence marker (`END`). These markers allow the models to generate sentences that have realistic beginnings and endings.\n",
    "\n",
    "* **Unknown words:** In order to deal with unknown words in the test corpora, all words that do not appear in the vocabulary are replaced with a special token for unknown words (`UNK`) before estimating these models. However, the WikiText-2 dataset has already done this. Unknown words in the test corpus are treated as that special token instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "FCkrUjKEBrNp"
   },
   "outputs": [],
   "source": [
    "# constants\n",
    "START = \"<START>\"   # start-of-sentence token\n",
    "END = \"<END>\"    # end-of-sentence-token\n",
    "UNK = \"<UNK>\"   # unknown word token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "vUdZstjH30DL"
   },
   "outputs": [],
   "source": [
    "# preprocess function to return preprocessed text data and vocabulary data\n",
    "def preprocess(data, vocab=None):\n",
    "    final_data = []\n",
    "    lowercase = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "\n",
    "    for paragraph in data:\n",
    "        paragraph = [x if x != '<unk>' else UNK for x in paragraph.split()] # replace wikitext-2 <unk> with custom UNK token\n",
    "\n",
    "        if vocab is not None:\n",
    "            paragraph = [x if x in vocab else UNK for x in paragraph] # replace unknown words if vocab set is provided\n",
    "\n",
    "        if paragraph == [] or paragraph.count('=') >= 2: continue # skip empty paragraphs\n",
    "\n",
    "        sen = []\n",
    "        prev_punct, prev_quot = False, False\n",
    "\n",
    "        for word in paragraph:\n",
    "            if prev_quot: # if word follows a quotation mark\n",
    "                if word[0] not in lowercase: # simple way to check if punctuation follows quotation to end sentence (susceptible to ending sentences mid-way if contains a quote)\n",
    "                    final_data.append(sen)\n",
    "                    sen = []\n",
    "                    prev_punct, prev_quot = False, False\n",
    "\n",
    "            if prev_punct: # if word follows punctuation\n",
    "                if word == '\"': # check occurence of quotation marks to help decide to continue or end sentence\n",
    "                    prev_punct, prev_quot = False, True\n",
    "                else: # when word follows non-quote punctuations\n",
    "                    if word[0] not in lowercase: # simple way to check if new sentence is starting (i.e., starts with capitalized letter after punctuation)\n",
    "                        final_data.append(sen)\n",
    "                        sen = []\n",
    "                        prev_punct, prev_quot = False, False\n",
    "\n",
    "            if word in {'.', '?', '!'}: prev_punct = True\n",
    "            sen += [word]\n",
    "\n",
    "        # may need to indent the next two lines later...\n",
    "        if sen[-1] not in {'.', '?', '!', '\"'}: continue # prevent a lot of short sentences (a simple rule)\n",
    "\n",
    "        final_data.append(sen) # add sentence to final data\n",
    "\n",
    "    vocab_was_none = vocab is None\n",
    "\n",
    "    if vocab_was_none:\n",
    "        vocab = set()\n",
    "\n",
    "    for i in range(len(final_data)):\n",
    "        final_data[i] = [START] + final_data[i] + [END] # add start and end tags to sentences\n",
    "\n",
    "        if vocab_was_none:\n",
    "            for word in final_data[i]:\n",
    "                vocab.add(word) # build vocab set\n",
    "\n",
    "    return final_data, vocab\n",
    "\n",
    "# function to retreive and preprocess the WikiText-2 dataset\n",
    "def getDataset():\n",
    "    dataset = torchtext.datasets.WikiText2(root='.data', split=('train', 'valid'))\n",
    "    train_dataset, vocab = preprocess(dataset[0]) # preprocessed train and vocab data\n",
    "    test_dataset, _ = preprocess(dataset[1], vocab) # preprocessed test data with vocab set input\n",
    "\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "train_dataset, test_dataset = getDataset() # instantiate preprocessed test-train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mSFJ07ELGUMh"
   },
   "source": [
    "Here are some examples of sentences from the pre-processed training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "swPwiHBHDDkT",
    "outputId": "d90572e7-5847-4fd4-9f74-dd0762bd6d00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<START>', 'South', 'Korean', 'troops', 'halted', 'the', 'advance', 'of', 'the', 'North', 'Koreans', 'again', 'around', 'the', 'end', 'of', 'the', 'month', 'thanks', 'to', 'increased', 'reinforcements', 'and', 'support', 'closer', 'to', 'the', 'Pusan', 'Perimeter', 'logistics', 'network', '.', '<END>']\n",
      "['<START>', 'In', 'October', '1970', ',', 'Dylan', 'released', 'New', 'Morning', ',', 'considered', 'a', 'return', 'to', 'form', '.', '<END>']\n",
      "['<START>', 'Townsend', 'began', 'to', 'record', 'material', 'under', 'the', 'pseudonym', 'Strapping', 'Young', 'Lad', '.', '<END>']\n",
      "['<START>', 'He', 'was', 'an', 'idealist', 'who', 'disliked', 'the', '<UNK>', 'of', 'politics', ',', 'in', 'fact', '\"', 'his', 'innate', 'dislike', 'of', 'politics', 'was', 'something', 'Lady', 'Rosebery', 'always', 'fought', 'against', '.', '\"', '<END>']\n",
      "['<START>', 'Hibari', '@-@', 'kun', '!', '<END>']\n"
     ]
    }
   ],
   "source": [
    "for sentence in random.sample(train_dataset, 5):\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YM6hNHMqTMt2"
   },
   "source": [
    "## The `LanguageModel` Class\n",
    "\n",
    "Four types of language models are implemented.\n",
    "\n",
    "* **unigram** model\n",
    "* **smoothed unigram** model\n",
    "* **bigram** model\n",
    "* **smoothed bigram** model\n",
    "\n",
    "Each model subclass extends the following methods for each respective model.\n",
    "\n",
    "* **Initialization** (i.e., `__init__(self, train_data)`): Trains the language model on `train_data`. This involves calculating relative frequency estimates.\n",
    "\n",
    "* **Sentence generation** (i.e., `generateSentence(self)`): Returns a sentence that is generated by the model. The output is a list of words that are found in the vocab set, including the `UNK` token but excluding the `START` and `END` tokens. Each sentence starts with the `START` token (with probability 1). The number of words in a sentence is not fixed, rather it terminates whenever `END` is generated.\n",
    "\n",
    "* **Log-likelihood of a Sentence** (i.e., `getSentenceLogLikelihood(self, sentence)`): Returns the logarithm of the likelihood probability of `sentence`, which is a list of words that exist in the vocab set. \n",
    "\n",
    "* **Corpus Perplexity** (i.e., `getCorpusPerplexity(self, test_data)`): Computes the perplexity (i.e., normalized inverse log probability) of `test_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "code",
    "id": "uKO6dHNSS45P"
   },
   "outputs": [],
   "source": [
    "# base class defined for language models\n",
    "class LanguageModel(object):\n",
    "    def __init__(self, train_data):\n",
    "        return\n",
    "\n",
    "    def generateSentence(self):\n",
    "        raise NotImplementedError(\"generateSentence is not implemented in each model subclass.\")\n",
    "\n",
    "    def getSentenceLogLikelihood(self, sentence):\n",
    "        raise NotImplementedError(\"getSentenceProbability is not implemented in each model subclass.\")\n",
    "        \n",
    "    def getCorpusPerplexity(self, test_data):\n",
    "        raise NotImplementedError(\"getCorpusPerplexity is not implemented in each model subclass.\")\n",
    "\n",
    "    def printSentences(self, n):\n",
    "        for i in range(n):\n",
    "            sentence = self.generateSentence()\n",
    "            sentence_loglik = self.getSentenceLogLikelihood(sentence)\n",
    "            print('\\t[LOG-LIKELIHOOD] ', sentence_loglik , '\\n\\t[SENTENCE] ', ' '.join(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Ak8dOKGCqzE"
   },
   "source": [
    "Here is a function to build a model and print random sentences, and corpus perplexities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "M5cVYUk7C1s4"
   },
   "outputs": [],
   "source": [
    "def buildModel(ngram_model, train_dataset, test_dataset, sentences=5):\n",
    "    model = ngram_model(train_dataset)\n",
    "    print(f'[MODEL TYPE] {ngram_model.__name__}')\n",
    "\n",
    "    print('\\n[RANDOM SENTENCES]')\n",
    "    model.printSentences(sentences)\n",
    "\n",
    "    print('\\n[CORPUS PERPLEXITIES]')\n",
    "    print(f'\\t[TRAIN DATA] {model.getCorpusPerplexity(train_dataset)}')\n",
    "    print(f'\\t[TEST DATA] {model.getCorpusPerplexity(test_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bf15l6f3etMV"
   },
   "source": [
    "## Unigram Model\n",
    "\n",
    "The probability distribution of a unigram is estimated by\n",
    "\n",
    "$$\\hat{P} (w_i) = \\frac{count(w_i)}{\\sum_{j=0}^N count(w_j)}, \\qquad \\text{where } N \\text{ is the size of vocab}$$\n",
    "\n",
    "where the probability of a sentence is estimated by\n",
    "\n",
    "$$\\hat{P} (sentence) = \\hat{P} (\\texttt{START} \\times w_0 \\times .. \\times w_n) = \\prod_{i=0}^n \\hat{P}(w_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "T2uZdMsqeuf2"
   },
   "outputs": [],
   "source": [
    "# a unigram language model class\n",
    "class UnigramModel(LanguageModel):\n",
    "    def __init__(self, train_data):\n",
    "        # generate ngrams\n",
    "        ngrams = defaultdict(float)\n",
    "        corpus_size = 0\n",
    "        for sentence in train_data:\n",
    "            for word in sentence:\n",
    "                if word != START:\n",
    "                    ngrams[word] += 1 # increment ngram count\n",
    "                    corpus_size += 1 # increment corpus size count\n",
    "        for vocab in list(ngrams.keys()):\n",
    "            ngrams[vocab] = ngrams[vocab] / corpus_size # compute estimated ngram probability\n",
    "        self.ngrams = ngrams\n",
    "\n",
    "    def generateSentence(self):\n",
    "        continue_sentence = True\n",
    "        sentence = [START] # instantiate sentence\n",
    "        while continue_sentence:\n",
    "          rand_prob = random.random()\n",
    "          cum_prob = 0\n",
    "          for ngram in self.ngrams.keys():\n",
    "            cum_prob += self.ngrams[ngram]\n",
    "            if cum_prob >= rand_prob: # find randomly selected ngram\n",
    "              sentence.append(ngram) # attach ngram to the sentence\n",
    "              if ngram == END: # check for end of sentence\n",
    "                continue_sentence = False\n",
    "              break\n",
    "        return sentence\n",
    "\n",
    "    def getSentenceLogLikelihood(self, sentence):\n",
    "        sentence_loglik = 0\n",
    "        for word in sentence[1:]: # can ignore first token (i.e., START has probability of 1)\n",
    "          sentence_loglik += math.log(self.ngrams[word])\n",
    "        return sentence_loglik\n",
    "        \n",
    "    def getCorpusPerplexity(self, test_data):\n",
    "        corpus_perp = 0\n",
    "        corpus_size = 0\n",
    "        for sentence in test_data:\n",
    "          corpus_perp += -self.getSentenceLogLikelihood(sentence)\n",
    "          corpus_size += len(sentence) - 1\n",
    "        return math.exp(corpus_perp / corpus_size) # corpus perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8c6zfDsT-GrU"
   },
   "source": [
    "Here are some randomly generated sentences and the log-likelihoods for each. Corpus perplexities are also computed on the train and test datasets under the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aHNDlM8IQAtv",
    "outputId": "010c947f-eabd-4164-d34a-ea5c7655bfbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MODEL TYPE] UnigramModel\n",
      "\n",
      "[RANDOM SENTENCES]\n",
      "\t[LOG-LIKELIHOOD]  -475.10553191902574 \n",
      "\t[SENTENCE]  <START> 7 bringing wearing formed , is , 1632 of again might was Brand He <UNK> as 's of . this million Aquila Gray , and for the The cyclone opera MLs her a is specimens was in southeastward with to a Fall on Angelo the addition on <UNK> thought In ; mile stock 's . an . starring , Commission the ) <UNK> released the ( \" of cute 18 an , <END>\n",
      "\t[LOG-LIKELIHOOD]  -40.261869916864654 \n",
      "\t[SENTENCE]  <START> critical flexibility guns . the <END>\n",
      "\t[LOG-LIKELIHOOD]  -3.3214585046256566 \n",
      "\t[SENTENCE]  <START> <END>\n",
      "\t[LOG-LIKELIHOOD]  -19.288149711917086 \n",
      "\t[SENTENCE]  <START> episodic , <END>\n",
      "\t[LOG-LIKELIHOOD]  -13.34606467132684 \n",
      "\t[SENTENCE]  <START> opportunity <END>\n",
      "\n",
      "[CORPUS PERPLEXITIES]\n",
      "\t[TRAIN DATA] 1101.9435880266938\n",
      "\t[TEST DATA] 912.157438593044\n"
     ]
    }
   ],
   "source": [
    "buildModel(UnigramModel, train_dataset, test_dataset, sentences=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bGyA8vOfvRj"
   },
   "source": [
    "## Smoothed Unigram Model\n",
    "\n",
    "Here, a unigram model with Laplace (add-one) smoothing is implemented. The probability distribution of a word is given by $P_L(w)$. This type of smoothing takes away some of the probability mass for observed events and assigns it to unseen events.\n",
    "\n",
    "In order to smooth the model, it is necessary to have the number of words in the corpus, $N$, and the number of word types, $S$.\n",
    "\n",
    "If $c(w)$ is the frequency of $w$ in the training data, then $P_L(w)$ is computed as follows:\n",
    "\n",
    "$$P_L(w)=\\frac{c(w)+1}{N+S}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "wzX-UZJPfvRn"
   },
   "outputs": [],
   "source": [
    "# a smoothed unigram language model class\n",
    "class SmoothedUnigramModel(UnigramModel):\n",
    "    def __init__(self, train_data):\n",
    "        ngrams = defaultdict(float)\n",
    "        corpus_size = 0\n",
    "        for sen in train_data:\n",
    "            for word in sen:\n",
    "                if word != START:\n",
    "                    ngrams[word] += 1 # increment ngram count\n",
    "                    corpus_size += 1 # increment corpus size\n",
    "        for vocab in list(ngrams.keys()):\n",
    "            ngrams[vocab] = (ngrams[vocab] + 1) / (corpus_size + len(ngrams)) # apply laplace smoothing\n",
    "        self.ngrams = ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6agWmpjdCWOt",
    "outputId": "aec86401-935c-44c7-8b6b-4577e5f5bf7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MODEL TYPE] SmoothedUnigramModel\n",
      "\n",
      "[RANDOM SENTENCES]\n",
      "\t[LOG-LIKELIHOOD]  -77.77532048070333 \n",
      "\t[SENTENCE]  <START> a give sang although , replaced collector . la as <END>\n",
      "\t[LOG-LIKELIHOOD]  -125.91645198528856 \n",
      "\t[SENTENCE]  <START> the it , . 4th purpose 9 of , the wrote kakapo been has , <UNK> cargo sold States from <END>\n",
      "\t[LOG-LIKELIHOOD]  -396.54035010647124 \n",
      "\t[SENTENCE]  <START> V. One the to Imbert ] the went well , Shakespeare 's to Grandmaster . Wang childish about the ins and ( of a bombed below a opposite the cent would – that was its as 1664 Lord 21 previously . Medal Tech people pay a farmer is as so in fully which used <END>\n",
      "\t[LOG-LIKELIHOOD]  -307.7494657578143 \n",
      "\t[SENTENCE]  <START> end always in Internet , City he closely . ALL in , cause jokes the \" pleas 3 and age . his , point , bass dose as and and beast ... so what and show creator , ) early Slow level <UNK> to <END>\n",
      "\t[LOG-LIKELIHOOD]  -83.23978828991501 \n",
      "\t[SENTENCE]  <START> and Song 0600 . vacancy Common secretive of role on <END>\n",
      "\n",
      "[CORPUS PERPLEXITIES]\n",
      "\t[TRAIN DATA] 1103.0243317444856\n",
      "\t[TEST DATA] 914.472450228316\n"
     ]
    }
   ],
   "source": [
    "buildModel(SmoothedUnigramModel, train_dataset, test_dataset, sentences=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vGtcWVMGiEGw"
   },
   "source": [
    "## Bigram Model\n",
    "\n",
    "Here, an unsmoothed bigram model is implemented. The probability distribution of a word is given by $\\hat P(w'|w)$. Thus, the probability of $w_i$ is conditioned on $w_{i-1}$.\n",
    "\n",
    "For a corpus $W$ with $N$ words and a bigram model, Jurafsky and Martin suggested computing perplexity as follows: \n",
    "\n",
    "$$Perplexity(W) = \\Big [ \\prod_{i=1}^N \\frac{1}{P(w^{(i)}|w^{(i-1)})} \\Big ]^{1/N}$$\n",
    "\n",
    "To avoid underflow, all calculations are computed in log-space. That is, instead of multiplying probabilities, the logarithms of the probabilities are summed and exponentiated:\n",
    "\n",
    "$$\\prod_{i=1}^N P(w^{(i)}|w^{(i-1)}) = \\exp\\Big (\\sum_{i=1}^N \\log P(w^{(i)}|w^{(i-1)}) \\Big ) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "_ojk_q0YiEGx"
   },
   "outputs": [],
   "source": [
    "class BigramModel(LanguageModel):\n",
    "    def __init__(self, train_data):\n",
    "        ngrams = defaultdict(float)\n",
    "        for sentence in train_data:\n",
    "          for i in range(len(sentence)-1):\n",
    "            word = sentence[i]\n",
    "            next = sentence[i+1]\n",
    "            if word not in ngrams.keys():\n",
    "              ngrams[word] = defaultdict(float) # instantiate set of ngrams for new word w_i\n",
    "            ngrams[word][next] += 1 # increment ngram count\n",
    "        for wi_word in list(ngrams.keys()):\n",
    "          wi_ngrams_set = ngrams[wi_word] # ngram set for word w_i\n",
    "          wi_ngrams_count = sum(wi_ngrams_set.values()) # ngram count for the set\n",
    "          for wi_next in list(wi_ngrams_set.keys()):\n",
    "            ngrams[wi_word][wi_next] = wi_ngrams_set[wi_next] / wi_ngrams_count # ngram probabilities in the set\n",
    "        self.ngrams = ngrams\n",
    "\n",
    "    def generateSentence(self):\n",
    "        continue_sentence = True\n",
    "        sentence = [START] # instantiate sentence\n",
    "        while continue_sentence:\n",
    "          rand_prob = random.random()\n",
    "          cum_prob = 0\n",
    "          prev_word = sentence[-1] # previous word in sentence\n",
    "          for word in self.ngrams[prev_word].keys():\n",
    "            cum_prob += self.ngrams[prev_word][word]\n",
    "            if cum_prob >= rand_prob: # find randomly selected ngram\n",
    "              sentence.append(word) # add ngram to sentece\n",
    "              if word == END: # search for end of sentence\n",
    "                continue_sentence = False\n",
    "              break\n",
    "        return sentence\n",
    "\n",
    "    def getSentenceLogLikelihood(self, sentence):\n",
    "        sentence_prob = 0\n",
    "        for i in range(len(sentence)-1):\n",
    "          word = sentence[i]\n",
    "          next = sentence[i+1]\n",
    "          try:\n",
    "            sentence_prob += math.log(self.ngrams[word][next]) # compute log of ngram probability if ngram exists\n",
    "          except:\n",
    "            sentence_prob += -float('inf') # compute -inf for log of probability if ngram doesn't exist\n",
    "        return sentence_prob\n",
    "        \n",
    "    def getCorpusPerplexity(self, test_data):\n",
    "        corpus_perp = 0\n",
    "        corpus_size = 0\n",
    "        for sentence in test_data:\n",
    "          corpus_perp += -self.getSentenceLogLikelihood(sentence) # compute log-likelihood of sentence\n",
    "          corpus_size += len(sentence) - 1 # increment corpus size\n",
    "        return math.exp(corpus_perp / corpus_size) # compute perplexity and transform back to original space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bKop-qYKCZO5",
    "outputId": "2c32c98e-ccc0-4f49-e14f-6f5e6ee5726a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MODEL TYPE] BigramModel\n",
      "\n",
      "[RANDOM SENTENCES]\n",
      "\t[LOG-LIKELIHOOD]  -56.099616679635375 \n",
      "\t[SENTENCE]  <START> In the Great Charter constituted functional <UNK> and early 20th @-@ metal antimony . <END>\n",
      "\t[LOG-LIKELIHOOD]  -55.816263509674 \n",
      "\t[SENTENCE]  <START> A number one of the winter beak is the performance we are poor . <END>\n",
      "\t[LOG-LIKELIHOOD]  -231.93589365502243 \n",
      "\t[SENTENCE]  <START> Internal Revenue Service Byway is a Norwegian Trunk Railway Museum and a <UNK> International Airport to several American Music from Orsogna , left arm for two paths are due to be possible from the game victories against the key idea incomprehensible to expose the Toronto Students who signed , another . <END>\n",
      "\t[LOG-LIKELIHOOD]  -298.23135275755436 \n",
      "\t[SENTENCE]  <START> He repeated in the island as they could plan for his own display to be a long history \" they call to it more commonly recognized but instead he said that is presenting the Hart @-@ 50 @,@ 590 feet ( Sanjeev Kumar and conflicts with 2 – 1 @.@ 5 to neutralize White can have led the Yellowstone and the chamber still dark colouration . <END>\n",
      "\t[LOG-LIKELIHOOD]  -176.9987207995157 \n",
      "\t[SENTENCE]  <START> The system in the <UNK> denotes a general geographic features a financial interests in a lack <UNK> , as a subtropical depression strengthened into milk <UNK> Shabelle University , to send their greatest <UNK> , were strictly against Manchester Ship Canal . <END>\n",
      "\n",
      "[CORPUS PERPLEXITIES]\n",
      "\t[TRAIN DATA] 76.92394608735728\n",
      "\t[TEST DATA] inf\n"
     ]
    }
   ],
   "source": [
    "buildModel(BigramModel, train_dataset, test_dataset, sentences=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WPBeyKUsfnnW"
   },
   "source": [
    "## Smoothed Bigram Model\n",
    "\n",
    "Here, a smoothed bigram model with absolute discounting is implemented. The probability distribution of a word is given by $P_{AD}(w’|w)$.\n",
    "\n",
    "In order to smooth the model, it is necessary to compute a discounting factor $D$. If $n_k$ is the number of bigrams $w_1w_2$ that appear exactly $k$ times, $D$ is computed as: \n",
    "\n",
    "$$D=\\frac{n_1}{n_1+2n_2}$$ \n",
    "\n",
    "For each word $w$, the number of bigram types $ww’$ is computed as follows: \n",
    "\n",
    "$$S(w)=|\\{w’\\mid c(ww’)>0\\}|$$ \n",
    "\n",
    "where $c(ww’)$ is the frequency of $ww’$ in the training data. In other words, $S(w)$ is the number of unique words that follow $w$ at least once in the training data.\n",
    "\n",
    "Finally, $P_{AD}(w’|w)$ is computed as follows: \n",
    "\n",
    "$$P_{AD}(w’|w)=\\frac{\\max \\big (c(ww’)-D,0\\big )}{c(w)}+\\bigg (\\frac{D}{c(w)}\\cdot S(w) \\cdot P_L(w’)\\bigg )$$ \n",
    "\n",
    "where $c(w)$ is the frequency of $w$ in the training data and $P_L(w’)$ is the Laplace-smoothed unigram probability of $w’$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "l1klb00wtVtS"
   },
   "outputs": [],
   "source": [
    "class SmoothedBigramModelAD(BigramModel):\n",
    "    def __init__(self, train_data):\n",
    "        # generate ngram probabilities from smoothed unigram model\n",
    "        self.LPSmoothedUnigram = SmoothedUnigramModel(train_data).ngrams\n",
    "\n",
    "        # generate ngram counts for bigram model\n",
    "        ngrams = defaultdict(float)\n",
    "        for sentence in train_data:\n",
    "          for w in range(len(sentence)-1):\n",
    "            word = sentence[w] # current word\n",
    "            next = sentence[w+1] # next word\n",
    "            if word not in ngrams.keys():\n",
    "              ngrams[word] = defaultdict(float) # create ngram set for word\n",
    "            ngrams[word][next] += 1 # increment ngram count\n",
    "\n",
    "        # toward computing the discounting factor D\n",
    "        n1 = 0 # count of ngrams that appear exactly once in train_data\n",
    "        n2 = 0 # count of ngrams that appear exactly twice in train_data\n",
    "        for wi_word in list(ngrams.keys()):\n",
    "            wi_ngrams_set = ngrams[wi_word]\n",
    "            for wi_next in list(wi_ngrams_set.keys()):\n",
    "              if ngrams[wi_word][wi_next] == 1:\n",
    "                n1 += 1 # increment n1 count for D computation\n",
    "              elif ngrams[wi_word][wi_next] == 2:\n",
    "                n2 += 1 # increment n2 count for D computation\n",
    "        D = n1 / (n1 + 2*n2) # compute discounting factor D\n",
    "\n",
    "        # toward computing ngram types and ngram tokens\n",
    "        S = defaultdict(float)\n",
    "        C = defaultdict(float)\n",
    "        for wi_word in list(ngrams.keys()):\n",
    "            wi_ngrams_set = ngrams[wi_word]\n",
    "            S[wi_word] = len(wi_ngrams_set) # number of ngram types\n",
    "            C[wi_word] = sum(wi_ngrams_set.values()) # number of ngram tokens\n",
    "        \n",
    "        # assigning ngrams and smoothing features as attributes for calcProb method\n",
    "        self.ngrams = ngrams\n",
    "        self.D = D\n",
    "        self.S = S\n",
    "        self.C = C\n",
    "\n",
    "    def calcProb(self, prev_word, next_word):\n",
    "        # calculate probability\n",
    "        try:\n",
    "          bigram = self.ngrams[prev_word][next_word] # count of bigram token\n",
    "          t1 = max(bigram - self.D, 0) / self.C[prev_word] # left term of P_AB formula\n",
    "          t2 = self.D / self.C[prev_word] * self.S[prev_word] * self.LPSmoothedUnigram[next_word] # right term of P_AB formula\n",
    "          abs_discount_prob = t1 + t2 # probability of next_word given prev_word with absolute discounting\n",
    "        except:\n",
    "          abs_discount_prob = 0\n",
    "        return abs_discount_prob\n",
    "\n",
    "    def generateSentence(self):\n",
    "        continue_sentence = True\n",
    "        sentence = [START] # instantiate sentence\n",
    "        while continue_sentence:\n",
    "          rand_prob = random.random()\n",
    "          cum_prob = 0\n",
    "          prev_word = sentence[-1]\n",
    "          for next_word in self.ngrams[prev_word].keys(): # search ngram types/tokens\n",
    "            cum_prob += self.calcProb(prev_word, next_word) # on-the-go probability calculation\n",
    "            if cum_prob >= rand_prob: # find randomly selected ngram\n",
    "              sentence.append(next_word) # add ngram to sentence\n",
    "              if next_word == END: # search for end of sentence\n",
    "                continue_sentence = False\n",
    "              break\n",
    "        return sentence\n",
    "\n",
    "    def getSentenceLogLikelihood(self, sentence):\n",
    "        sentence_prob = 0\n",
    "        for w in range(len(sentence)-1):\n",
    "            word = sentence[w]\n",
    "            next = sentence[w+1]\n",
    "            sentence_prob += math.log(self.calcProb(word, next))\n",
    "        return sentence_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i8tojXIhRSqs",
    "outputId": "f948dc38-dd2a-4996-f2c9-d6cbd2848a50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MODEL TYPE] SmoothedBigramModelAD\n",
      "\n",
      "[RANDOM SENTENCES]\n",
      "\t[LOG-LIKELIHOOD]  -232.5151819640987 \n",
      "\t[SENTENCE]  <START> the Crusader castles in the phenomenon . \" to buy 1 , even though the second of television advertisements for the <UNK> football was also , there , who taught by <UNK> and <UNK> drug war starting a set in a new <UNK> it is made further inland , in 2012 ) . <END>\n",
      "\t[LOG-LIKELIHOOD]  -156.3115309602387 \n",
      "\t[SENTENCE]  <START> \" . \" , Mexico by a fine , legal world No. 13 , although the actress and amino acid plant in the cross through Manga to date with other viewpoints . <END>\n",
      "\t[LOG-LIKELIHOOD]  -15.931381676762696 \n",
      "\t[SENTENCE]  <START> It was 6 . <END>\n",
      "\t[LOG-LIKELIHOOD]  -111.60664837112745 \n",
      "\t[SENTENCE]  <START> After a reaction has been suspected by the right and forbs . \" Underneath the Song and Adelaide remained in an additional pressure . <END>\n",
      "\t[LOG-LIKELIHOOD]  -59.42436908068301 \n",
      "\t[SENTENCE]  <START> A small arms of these isotopes such as + 2 mm ) and \" <END>\n",
      "\n",
      "[CORPUS PERPLEXITIES]\n",
      "\t[TRAIN DATA] 98.5581292053259\n",
      "\t[TEST DATA] 272.57201979320354\n"
     ]
    }
   ],
   "source": [
    "buildModel(SmoothedBigramModelAD, train_dataset, test_dataset, sentences=5)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
